{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Initial Libraries and Data Set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('data/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21597 entries, 0 to 21596\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21597 non-null  int64  \n",
      " 1   date           21597 non-null  object \n",
      " 2   price          21597 non-null  float64\n",
      " 3   bedrooms       21597 non-null  int64  \n",
      " 4   bathrooms      21597 non-null  float64\n",
      " 5   sqft_living    21597 non-null  int64  \n",
      " 6   sqft_lot       21597 non-null  int64  \n",
      " 7   floors         21597 non-null  float64\n",
      " 8   waterfront     19221 non-null  float64\n",
      " 9   view           21534 non-null  float64\n",
      " 10  condition      21597 non-null  int64  \n",
      " 11  grade          21597 non-null  int64  \n",
      " 12  sqft_above     21597 non-null  int64  \n",
      " 13  sqft_basement  21597 non-null  object \n",
      " 14  yr_built       21597 non-null  int64  \n",
      " 15  yr_renovated   17755 non-null  float64\n",
      " 16  zipcode        21597 non-null  int64  \n",
      " 17  lat            21597 non-null  float64\n",
      " 18  long           21597 non-null  float64\n",
      " 19  sqft_living15  21597 non-null  int64  \n",
      " 20  sqft_lot15     21597 non-null  int64  \n",
      "dtypes: float64(8), int64(11), object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert NA values in 'waterfront' to 0\n",
    "\n",
    "data['view'] = data['view'].replace(np.nan, 0)\n",
    "\n",
    "#Convert NA values in 'waterfront' to 0\n",
    "\n",
    "data['waterfront'] = data['waterfront'].replace(np.nan, 0)\n",
    "\n",
    "#Convert NA values in 'yr_renovated' to 0\n",
    "\n",
    "data['yr_renovated'] = data['yr_renovated'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "date             0\n",
       "price            0\n",
       "bedrooms         0\n",
       "bathrooms        0\n",
       "sqft_living      0\n",
       "sqft_lot         0\n",
       "floors           0\n",
       "waterfront       0\n",
       "view             0\n",
       "condition        0\n",
       "grade            0\n",
       "sqft_above       0\n",
       "sqft_basement    0\n",
       "yr_built         0\n",
       "yr_renovated     0\n",
       "zipcode          0\n",
       "lat              0\n",
       "long             0\n",
       "sqft_living15    0\n",
       "sqft_lot15       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21597 entries, 0 to 21596\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   id             21597 non-null  int64  \n",
      " 1   date           21597 non-null  object \n",
      " 2   price          21597 non-null  float64\n",
      " 3   bedrooms       21597 non-null  int64  \n",
      " 4   bathrooms      21597 non-null  float64\n",
      " 5   sqft_living    21597 non-null  int64  \n",
      " 6   sqft_lot       21597 non-null  int64  \n",
      " 7   floors         21597 non-null  float64\n",
      " 8   waterfront     21597 non-null  float64\n",
      " 9   view           21597 non-null  float64\n",
      " 10  condition      21597 non-null  int64  \n",
      " 11  grade          21597 non-null  int64  \n",
      " 12  sqft_above     21597 non-null  int64  \n",
      " 13  sqft_basement  21597 non-null  object \n",
      " 14  yr_built       21597 non-null  int64  \n",
      " 15  yr_renovated   21597 non-null  float64\n",
      " 16  zipcode        21597 non-null  int64  \n",
      " 17  lat            21597 non-null  float64\n",
      " 18  long           21597 non-null  float64\n",
      " 19  sqft_living15  21597 non-null  int64  \n",
      " 20  sqft_lot15     21597 non-null  int64  \n",
      "dtypes: float64(8), int64(11), object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert zipcode to a str\n",
    "\n",
    "data['zipcode_str'] = data['zipcode'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restructure yr_built into a continuous variable called 'age'\n",
    "\n",
    "data['age'] = data['yr_built'].apply(lambda x: 2021 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 'ever_refurbished' to reflect whether or not a house has even been refurbished\n",
    "\n",
    "data['refurbished'] = np.where((data['yr_renovated'] > 0),\"Yes\",\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish new Cont and Cat data\n",
    "\n",
    "cont_data = ['price',\n",
    " 'sqft_living',\n",
    " 'sqft_lot',\n",
    " 'grade',\n",
    " 'sqft_above',\n",
    " 'sqft_living15',\n",
    " 'sqft_lot15',\n",
    " 'age']\n",
    "\n",
    "cat_data = [\n",
    " 'bedrooms',\n",
    " 'bathrooms',\n",
    " 'floors',\n",
    " 'waterfront',\n",
    " 'condition',\n",
    " 'zipcode_str',\n",
    " 'refurbished', 'view']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.17925043e-01,  1.37093240e-01,  1.81110116e-01,  1.39407756e-01,\n",
       "        1.00682102e-01, -2.77627237e-02, -3.43461166e-02, -2.73509983e-02,\n",
       "        6.67635369e-02, -6.65372304e-02,  9.26980351e-01,  1.11390282e-01,\n",
       "        1.25592154e-01, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,\n",
       "       -1.08538967e+10, -1.08538967e+10, -1.08538967e+10,  4.37729309e+10,\n",
       "        4.37729309e+10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recreate Features Data Set Model 5\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "#One Hot (So Hawt) Encode\n",
    "\n",
    "data_ohe = pd.get_dummies(data[cat_data])\n",
    "\n",
    "#Create Cont Dataset\n",
    "\n",
    "data_cont = data[cont_data]\n",
    "\n",
    "# #Log Data\n",
    "\n",
    "logged_features = []\n",
    "\n",
    "for item in list(data_cont.columns):\n",
    "    data_cont[f'{item}_log'] = np.log(data_cont[item])\n",
    "    logged_features.append(f'{item}_log')\n",
    "    \n",
    "scaled_features = []\n",
    "for item in logged_features:\n",
    "    data_cont[f'{item}_scaled'] = StandardScaler().fit_transform(data_cont[item].values.reshape(-1, 1))\n",
    "    scaled_features.append(f'{item}_scaled')\n",
    "\n",
    "#Create Features DataFrame\n",
    "    \n",
    "features_df = pd.concat([data_cont[scaled_features], data_ohe], axis = 1)\n",
    "\n",
    "#Establish X and y\n",
    "\n",
    "X = features_df\n",
    "X = X.drop(['price_log_scaled'], axis = 1)\n",
    "y = features_df['price_log_scaled']\n",
    "\n",
    "#Instantiate Linear Regression and execute Cross Val Score for Model Validation\n",
    "\n",
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X, y)\n",
    "\n",
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Polynomial Dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "#Create a copy of X\n",
    "\n",
    "X2 = X.copy()\n",
    "\n",
    "#take out zipcodes to lessen iteration\n",
    "\n",
    "non_zipcode_features = []\n",
    "\n",
    "for item in X2.columns:\n",
    "    if 'zip' not in item:\n",
    "        non_zipcode_features.append(item)\n",
    "\n",
    "#Create polynomial features\n",
    "\n",
    "X2_poly = poly.fit_transform(X2[non_zipcode_features])\n",
    "\n",
    "#Get polynomial column names\n",
    "\n",
    "X2_poly_column_names = poly.get_feature_names(input_features = X.columns)\n",
    "\n",
    "#create polynomial features Data Frame\n",
    "\n",
    "X2_poly_df = pd.DataFrame(X2_poly)\n",
    "\n",
    "#add column names to polynomial features Data Frame\n",
    "\n",
    "X2_poly_df.columns = list(X2_poly_column_names)\n",
    "\n",
    "#Isolate New Features\n",
    "\n",
    "new_features = []\n",
    "\n",
    "for item in X2_poly_df.columns:\n",
    "    if item not in X.columns:\n",
    "        new_features.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8790571010201799\n",
      "sqft_living_log_scaled^2 0.8811306274109677\n",
      "sqft_living_log_scaled sqft_lot_log_scaled 0.8792222080721113\n",
      "sqft_living_log_scaled grade_log_scaled 0.8827146764907257\n",
      "sqft_living_log_scaled sqft_above_log_scaled 0.8815795688229325\n",
      "sqft_living_log_scaled sqft_living15_log_scaled 0.8800274931651556\n",
      "sqft_living_log_scaled sqft_lot15_log_scaled 0.8791590219492511\n",
      "sqft_living_log_scaled age_log_scaled 0.8823515652215642\n",
      "sqft_living_log_scaled bedrooms 0.8796140058586351\n",
      "sqft_living_log_scaled bathrooms 0.8813605812339199\n",
      "sqft_living_log_scaled floors 0.8819768798491376\n",
      "sqft_living_log_scaled waterfront 0.8791123510859189\n",
      "sqft_living_log_scaled condition 0.8794448962620922\n",
      "sqft_living_log_scaled view 0.8790420552016783\n",
      "sqft_living_log_scaled zipcode_str_98001 0.87912395437923\n",
      "sqft_living_log_scaled zipcode_str_98002 0.8791244743615934\n",
      "sqft_lot_log_scaled^2 0.879319170247882\n",
      "sqft_lot_log_scaled grade_log_scaled 0.8790575548223337\n",
      "sqft_lot_log_scaled sqft_above_log_scaled 0.8790823053395252\n",
      "sqft_lot_log_scaled sqft_living15_log_scaled 0.8795287421372013\n",
      "sqft_lot_log_scaled sqft_lot15_log_scaled 0.8794450469034866\n",
      "sqft_lot_log_scaled age_log_scaled 0.8802640475066077\n",
      "sqft_lot_log_scaled bedrooms 0.8793692349021089\n",
      "sqft_lot_log_scaled bathrooms 0.879168881998528\n",
      "sqft_lot_log_scaled floors 0.8799940499007886\n",
      "sqft_lot_log_scaled waterfront 0.8791542060111437\n",
      "sqft_lot_log_scaled condition 0.8793345009297153\n",
      "sqft_lot_log_scaled view 0.8791126809793743\n",
      "sqft_lot_log_scaled zipcode_str_98001 0.8790246991267929\n",
      "sqft_lot_log_scaled zipcode_str_98002 0.8790246107896478\n",
      "grade_log_scaled^2 0.8816732394096641\n",
      "grade_log_scaled sqft_above_log_scaled 0.8827504703134753\n",
      "grade_log_scaled sqft_living15_log_scaled 0.8805293047297722\n",
      "grade_log_scaled sqft_lot15_log_scaled 0.8790582385988926\n",
      "grade_log_scaled age_log_scaled 0.8808997368242311\n",
      "grade_log_scaled bedrooms 0.8807192797236585\n",
      "grade_log_scaled bathrooms 0.8816702229826312\n",
      "grade_log_scaled floors 0.8801932211838139\n",
      "grade_log_scaled waterfront 0.879078119850633\n",
      "grade_log_scaled condition 0.8793072302937446\n",
      "grade_log_scaled view 0.8791109759088835\n",
      "grade_log_scaled zipcode_str_98001 0.8791629922905548\n",
      "grade_log_scaled zipcode_str_98002 0.8791633408202859\n",
      "sqft_above_log_scaled^2 0.8813249436189894\n",
      "sqft_above_log_scaled sqft_living15_log_scaled 0.8802280047551768\n",
      "sqft_above_log_scaled sqft_lot15_log_scaled 0.8790824141893447\n",
      "sqft_above_log_scaled age_log_scaled 0.8823966801985821\n",
      "sqft_above_log_scaled bedrooms 0.8796826966746634\n",
      "sqft_above_log_scaled bathrooms 0.8814858694354823\n",
      "sqft_above_log_scaled floors 0.8817160677766843\n",
      "sqft_above_log_scaled waterfront 0.8790828333709342\n",
      "sqft_above_log_scaled condition 0.8795488001627978\n",
      "sqft_above_log_scaled view 0.8790582904895196\n",
      "sqft_above_log_scaled zipcode_str_98001 0.8790489333717135\n",
      "sqft_above_log_scaled zipcode_str_98002 0.8790489255653865\n",
      "sqft_living15_log_scaled^2 0.8797577114754697\n",
      "sqft_living15_log_scaled sqft_lot15_log_scaled 0.8794076211596558\n",
      "sqft_living15_log_scaled age_log_scaled 0.880493456621927\n",
      "sqft_living15_log_scaled bedrooms 0.8792453143382355\n",
      "sqft_living15_log_scaled bathrooms 0.8802433510375576\n",
      "sqft_living15_log_scaled floors 0.8805095635057538\n",
      "sqft_living15_log_scaled waterfront 0.8790844347430763\n",
      "sqft_living15_log_scaled condition 0.8791434695174735\n",
      "sqft_living15_log_scaled view 0.8790330439285157\n",
      "sqft_living15_log_scaled zipcode_str_98001 0.8790455029126646\n",
      "sqft_living15_log_scaled zipcode_str_98002 0.8790452879338341\n",
      "sqft_lot15_log_scaled^2 0.8794536542470753\n",
      "sqft_lot15_log_scaled age_log_scaled 0.8803899787488273\n",
      "sqft_lot15_log_scaled bedrooms 0.8793404044089422\n",
      "sqft_lot15_log_scaled bathrooms 0.8791923612407073\n",
      "sqft_lot15_log_scaled floors 0.8800027592399046\n",
      "sqft_lot15_log_scaled waterfront 0.8791465385117385\n",
      "sqft_lot15_log_scaled condition 0.8792896829264187\n",
      "sqft_lot15_log_scaled view 0.8790685092205507\n",
      "sqft_lot15_log_scaled zipcode_str_98001 0.8790295009193958\n",
      "sqft_lot15_log_scaled zipcode_str_98002 0.8790293910105843\n",
      "age_log_scaled^2 0.8820420077623826\n",
      "age_log_scaled bedrooms 0.8799265172982969\n",
      "age_log_scaled bathrooms 0.8808526386778721\n",
      "age_log_scaled floors 0.8790108377264474\n",
      "age_log_scaled waterfront 0.8791722854262571\n",
      "age_log_scaled condition 0.8804585049580534\n",
      "age_log_scaled view 0.8790400474210939\n",
      "age_log_scaled zipcode_str_98001 0.879024347992018\n",
      "age_log_scaled zipcode_str_98002 0.8790241775123612\n",
      "bedrooms^2 0.8790213829326483\n",
      "bedrooms bathrooms 0.8794032351783676\n",
      "bedrooms floors 0.8803795744616354\n",
      "bedrooms waterfront 0.8790648268574163\n",
      "bedrooms condition 0.8790732042979373\n",
      "bedrooms view 0.8790234656564507\n",
      "bedrooms zipcode_str_98001 0.8790827851665425\n",
      "bedrooms zipcode_str_98002 0.8790823178558022\n",
      "bathrooms^2 0.8798156675467081\n",
      "bathrooms floors 0.8795321186930766\n",
      "bathrooms waterfront 0.8791029174232943\n",
      "bathrooms condition 0.8793057839651999\n",
      "bathrooms view 0.8790225076502832\n",
      "bathrooms zipcode_str_98001 0.8791418660812909\n",
      "bathrooms zipcode_str_98002 0.8791410929861062\n",
      "floors^2 0.8795622952970261\n",
      "floors waterfront 0.8790249728439438\n",
      "floors condition 0.8791155126005945\n",
      "floors view 0.879061709216186\n",
      "floors zipcode_str_98001 0.8790510395367299\n",
      "floors zipcode_str_98002 0.8790515746604364\n",
      "waterfront^2 0.8790561031820809\n",
      "waterfront condition 0.8790190291641882\n",
      "waterfront view 0.8790473762994004\n",
      "waterfront zipcode_str_98001 0.8790554253825202\n",
      "waterfront zipcode_str_98002 0.8790550130102581\n",
      "condition^2 0.8790306866265138\n",
      "condition view 0.8790838437584287\n",
      "condition zipcode_str_98001 0.8790868139820172\n",
      "condition zipcode_str_98002 0.8790868388041467\n",
      "view^2 0.8790664083269206\n",
      "view zipcode_str_98001 0.8790319069639116\n",
      "view zipcode_str_98002 0.8790315881379135\n",
      "zipcode_str_98001^2 0.8790568178251554\n",
      "zipcode_str_98001 zipcode_str_98002 0.8790571010201799\n",
      "zipcode_str_98002^2 0.8790568123558534\n"
     ]
    }
   ],
   "source": [
    "#Locate Useful Features from Polynomial Data Set\n",
    "\n",
    "useful_features = []\n",
    "useful_scores = []\n",
    "\n",
    "for item in new_features:\n",
    "    \n",
    "    x_improve = pd.concat([X, X2_poly_df[item]], axis = 1)\n",
    "\n",
    "    #Instantiate Linear Regression and execute Cross Val Score for Model Validation to establish baseline\n",
    "    \n",
    "    new_score = np.mean(cross_val_score(linreg, x_improve, y, cv = 10, scoring='r2'))\n",
    "\n",
    "    if new_score > baseline:\n",
    "        useful_features.append(item)\n",
    "        useful_scores.append(new_score)\n",
    "    \n",
    "    print(item, new_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_log_scaled sqft_above_log_scaled']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting out top_20_features\n",
    "\n",
    "potential_features = pd.DataFrame(useful_features)\n",
    "\n",
    "potential_features['useful_scores'] = useful_scores\n",
    "\n",
    "potential_features.columns =['potential_features', 'useful_scores']\n",
    "\n",
    "top_features = list(potential_features.sort_values(by = 'useful_scores', ascending = False).head(1)\n",
    "                       ['potential_features'])\n",
    "\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8790568123558534"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Establish improved baseline score with all new features\n",
    "\n",
    "X_improved = pd.concat([X, X2_poly_df[useful_features]], axis = 1)\n",
    "\n",
    "np.mean(cross_val_score(linreg, x_improve, y, cv = 10, scoring='r2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8827504703134753"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Estimate effect of only using the top features\n",
    "\n",
    "X_improved = pd.concat([X, X2_poly_df[top_features]], axis = 1)\n",
    "\n",
    "np.mean(cross_val_score(linreg, X_improved, y, cv = 10, scoring='r2'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions and Polynomial Analysis\n",
    "\n",
    "After having iterated through the model several times, it is clear that the only meaningful iteration is\n",
    "the relationship between Grade and Square Footage. This absolutely makes sense as the quality and size of the\n",
    "house (apart from its location) is a meaningful interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weeding out useless features\n",
    "\n",
    "X_improved.columns\n",
    "\n",
    "#Isolating non Zip Code Columns\n",
    "\n",
    "non_zip_features = []\n",
    "zip_features = []\n",
    "\n",
    "for item in X_improved.columns:\n",
    "    if 'zip' not in item:\n",
    "        non_zip_features.append(item)\n",
    "    else:\n",
    "        zip_features.append(item)\n",
    "        \n",
    "non_zip_features\n",
    "\n",
    "#Determining the effect of removing each feature from the model\n",
    "\n",
    "isolated_item = []\n",
    "score_without_item = []\n",
    "\n",
    "\n",
    "for item in non_zip_features:\n",
    "    isolated_features = list(X_improved.columns)\n",
    "    isolated_features.remove(item)\n",
    "    isolated_score = np.mean(cross_val_score(linreg, X_improved[isolated_features], y, cv = 10, scoring= 'r2'))\n",
    "    isolated_item.append(item)\n",
    "    score_without_item.append(isolated_score)\n",
    "\n",
    "    \n",
    "isolated_item = pd.DataFrame(isolated_item)\n",
    "score_without_item = pd.DataFrame(score_without_item)\n",
    "isolation_trial_df = pd.concat([isolated_item, score_without_item], axis = 1)\n",
    "isolation_trial_df.columns = ['Feature', 'Model_Score_Without_Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8802775734439958\n",
      "                                   Feature  Model_Score_Without_Feature  Keep\n",
      "0                   sqft_living_log_scaled                     0.875372  Keep\n",
      "1                      sqft_lot_log_scaled                     0.880496  Keep\n",
      "2                         grade_log_scaled                     0.874439  Keep\n",
      "3                    sqft_above_log_scaled                     0.880359  Keep\n",
      "4                 sqft_living15_log_scaled                     0.879960  Keep\n",
      "10                              waterfront                     0.878093  Keep\n",
      "11                               condition                     0.877999  Keep\n",
      "12                                    view                     0.876692  Keep\n",
      "15  grade_log_scaled sqft_above_log_scaled                     0.879057  Keep\n"
     ]
    }
   ],
   "source": [
    "#Isolate Core Features With Real Effect on Model\n",
    "\n",
    "isolation_trial_df['Keep'] = np.where(isolation_trial_df['Model_Score_Without_Feature'] <=0.882, 'Keep', 'Discard')\n",
    "\n",
    "core_features = list(isolation_trial_df[isolation_trial_df['Keep'] == 'Keep']['Feature'])\n",
    "\n",
    "core_features.extend(zip_features)\n",
    "\n",
    "print(np.mean(cross_val_score(linreg, X_improved[core_features], y, cv = 10)))\n",
    "\n",
    "print(isolation_trial_df[isolation_trial_df['Keep'] == 'Keep'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Scores:\n",
      "R2: 0.8826441207194121\n",
      "Mean Absolute Error: 0.25194560230926816\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "linreg.fit(X_improved[core_features], y)\n",
    "\n",
    "y_train_pred = linreg.predict(X_improved[core_features])\n",
    "\n",
    "print(\"Training Scores:\")\n",
    "print(f\"R2: {r2_score(y, y_train_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mean_absolute_error(y, y_train_pred)}\")\n",
    "# print(\"---\")\n",
    "# print(\"Testing Scores:\")\n",
    "# print(f\"R2: {r2_score(y_test_zip, y_test_pred)}\")\n",
    "# print(f\"Mean Absolute Error: {mean_absolute_error(y_test_zip, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_sample = data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-035b57aaf120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# normalizing... can just use our scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mforecast_cont_log_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_cont_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mforecast_cont_log_scaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mCopy\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \"\"\"\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "forecast_cont = forecast_sample[cont_data]\n",
    "\n",
    "# log features\n",
    "\n",
    "log_names = [f'{column}_log' for column in forecast_cont.columns]\n",
    "\n",
    "forecast_cont_log = np.log(forecast_cont.astype(float)) # won't work unless float\n",
    "forecast_cont_log.columns = log_names\n",
    "\n",
    "# normalizing... can just use our scaler\n",
    "\n",
    "forecast_cont_log_scaled = scaler.transform(forecast_cont_log)\n",
    "\n",
    "forecast_cont_log_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate Features Data Set Model 5\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "#One Hot (So Hawt) Encode (for forecast)\n",
    "\n",
    "forecast_ohe = pd.get_dummies(forecast_sample[cat_data])\n",
    "\n",
    "#Create Cont Dataset (for forecast)\n",
    "\n",
    "forecast_cont = forecast_sample[cont_data]\n",
    "\n",
    "# #Log Data\n",
    "\n",
    "forecast_logged_features = []\n",
    "\n",
    "for item in list(forecast_cont.columns):\n",
    "    forecast_cont[f'{item}_log'] = np.log(forecast_cont[item])\n",
    "    forecast_logged_features.append(f'{item}_log')\n",
    "    \n",
    "forecast_scaled_features = []\n",
    "for item in forecast_logged_features:\n",
    "    forecast_cont[f'{item}_scaled'] = StandardScaler().fit_transform(forecast_cont[item].values.reshape(-1, 1))\n",
    "    forecast_scaled_features.append(f'{item}_scaled')\n",
    "\n",
    "#Create Features DataFrame\n",
    "    \n",
    "forecast_features_df = pd.concat([forecast_cont[scaled_features], forecast_ohe], axis = 1)\n",
    "\n",
    "# #Establish X and y\n",
    "\n",
    "# X = forecast_features_df\n",
    "# X = X.drop(['price_log_scaled'], axis = 1)\n",
    "# y = features_df['price_log_scaled']\n",
    "\n",
    "# #Instantiate Linear Regression and execute Cross Val Score for Model Validation\n",
    "\n",
    "# linreg = LinearRegression()\n",
    "\n",
    "# baseline = np.mean(cross_val_score(linreg, X, y, cv = 10, scoring='r2'))\n",
    "\n",
    "# baseline\n",
    "\n",
    "forecast_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
